{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.',\n",
       " 'It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we want to split the paragraph into sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "paragraph = \"\"\"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities. \"\"\"\n",
    "\n",
    "sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " ',',\n",
       " 'or',\n",
       " 'more',\n",
       " 'commonly',\n",
       " 'NLTK',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'libraries',\n",
       " 'and',\n",
       " 'programs',\n",
       " 'for',\n",
       " 'symbolic',\n",
       " 'and',\n",
       " 'statistical',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'for',\n",
       " 'English',\n",
       " 'written',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language',\n",
       " '.',\n",
       " 'It',\n",
       " 'supports',\n",
       " 'classification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'stemming',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " 'functionalities',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate contractions\n",
    "word_tokenize(\"can't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_8172\\4274103910.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokenizer = RegexpTokenizer(\"[\\w']+\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't\n",
      "contraction\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "\n",
    "## using comprihansion\n",
    "# [word for word in words if word not in english_stopwords]\n",
    "\n",
    "#using loop \n",
    "for word in words:\n",
    "  if word not in english_stopwords:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aadi',\n",
       " 'aaj',\n",
       " 'aap',\n",
       " 'aapne',\n",
       " 'aata',\n",
       " 'aati',\n",
       " 'aaya',\n",
       " 'aaye',\n",
       " 'ab',\n",
       " 'abbe',\n",
       " 'abbey',\n",
       " 'abe',\n",
       " 'abhi',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'accha',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'acha',\n",
       " 'achcha',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agar',\n",
       " 'ain',\n",
       " 'aint',\n",
       " \"ain't\",\n",
       " 'aisa',\n",
       " 'aise',\n",
       " 'aisi',\n",
       " 'alag',\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'andar',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'ap',\n",
       " 'apan',\n",
       " 'apart',\n",
       " 'apna',\n",
       " 'apnaa',\n",
       " 'apne',\n",
       " 'apni',\n",
       " 'appear',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'arent',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'arre',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'at',\n",
       " 'aur',\n",
       " 'avum',\n",
       " 'aya',\n",
       " 'aye',\n",
       " 'baad',\n",
       " 'baar',\n",
       " 'bad',\n",
       " 'bahut',\n",
       " 'bana',\n",
       " 'banae',\n",
       " 'banai',\n",
       " 'banao',\n",
       " 'banaya',\n",
       " 'banaye',\n",
       " 'banayi',\n",
       " 'banda',\n",
       " 'bande',\n",
       " 'bandi',\n",
       " 'bane',\n",
       " 'bani',\n",
       " 'bas',\n",
       " 'bata',\n",
       " 'batao',\n",
       " 'bc',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bhai',\n",
       " 'bheetar',\n",
       " 'bhi',\n",
       " 'bhitar',\n",
       " 'bht',\n",
       " 'bilkul',\n",
       " 'bohot',\n",
       " 'bol',\n",
       " 'bola',\n",
       " 'bole',\n",
       " 'boli',\n",
       " 'bolo',\n",
       " 'bolta',\n",
       " 'bolte',\n",
       " 'bolti',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'bro',\n",
       " 'btw',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chahiye',\n",
       " 'chaiye',\n",
       " 'chal',\n",
       " 'chalega',\n",
       " 'chhaiye',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'couldnt',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'de',\n",
       " 'dede',\n",
       " 'dega',\n",
       " 'degi',\n",
       " 'dekh',\n",
       " 'dekha',\n",
       " 'dekhe',\n",
       " 'dekhi',\n",
       " 'dekho',\n",
       " 'denge',\n",
       " 'dhang',\n",
       " 'di',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'didnt',\n",
       " \"didn't\",\n",
       " 'dijiye',\n",
       " 'diya',\n",
       " 'diyaa',\n",
       " 'diye',\n",
       " 'diyo',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " 'dono',\n",
       " 'dont',\n",
       " \"don't\",\n",
       " 'doosra',\n",
       " 'doosre',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'dude',\n",
       " 'dunga',\n",
       " 'dungi',\n",
       " 'during',\n",
       " 'dusra',\n",
       " 'dusre',\n",
       " 'dusri',\n",
       " 'dvaara',\n",
       " 'dvara',\n",
       " 'dwaara',\n",
       " 'dwara',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'ek',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'fir',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'gaya',\n",
       " 'gaye',\n",
       " 'gayi',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'ghar',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'haan',\n",
       " 'had',\n",
       " 'hadd',\n",
       " 'hadn',\n",
       " 'hadnt',\n",
       " \"hadn't\",\n",
       " 'hai',\n",
       " 'hain',\n",
       " 'hamara',\n",
       " 'hamare',\n",
       " 'hamari',\n",
       " 'hamne',\n",
       " 'han',\n",
       " 'happens',\n",
       " 'har',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'hasn',\n",
       " 'hasnt',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " 'havent',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hm',\n",
       " 'hmm',\n",
       " 'ho',\n",
       " 'hoga',\n",
       " 'hoge',\n",
       " 'hogi',\n",
       " 'hona',\n",
       " 'honaa',\n",
       " 'hone',\n",
       " 'honge',\n",
       " 'hongi',\n",
       " 'honi',\n",
       " 'hopefully',\n",
       " 'hota',\n",
       " 'hotaa',\n",
       " 'hote',\n",
       " 'hoti',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hoyenge',\n",
       " 'hoyengi',\n",
       " 'hu',\n",
       " 'hua',\n",
       " 'hue',\n",
       " 'huh',\n",
       " 'hui',\n",
       " 'hum',\n",
       " 'humein',\n",
       " 'humne',\n",
       " 'hun',\n",
       " 'huye',\n",
       " 'huyi',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'idk',\n",
       " 'ie',\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'imo',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'inhe',\n",
       " 'inhi',\n",
       " 'inho',\n",
       " 'inka',\n",
       " 'inkaa',\n",
       " 'inke',\n",
       " 'inki',\n",
       " 'inn',\n",
       " 'inner',\n",
       " 'inse',\n",
       " 'insofar',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " 'ise',\n",
       " 'isi',\n",
       " 'iska',\n",
       " 'iskaa',\n",
       " 'iske',\n",
       " 'iski',\n",
       " 'isme',\n",
       " 'isn',\n",
       " 'isne',\n",
       " 'isnt',\n",
       " \"isn't\",\n",
       " 'iss',\n",
       " 'isse',\n",
       " 'issi',\n",
       " 'isski',\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'itna',\n",
       " 'itne',\n",
       " 'itni',\n",
       " 'itno',\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " 'ityaadi',\n",
       " 'ityadi',\n",
       " \"i've\",\n",
       " 'ja',\n",
       " 'jaa',\n",
       " 'jab',\n",
       " 'jabh',\n",
       " 'jaha',\n",
       " 'jahaan',\n",
       " 'jahan',\n",
       " 'jaisa',\n",
       " 'jaise',\n",
       " 'jaisi',\n",
       " 'jata',\n",
       " 'jayega',\n",
       " 'jidhar',\n",
       " 'jin',\n",
       " 'jinhe',\n",
       " 'jinhi',\n",
       " 'jinho',\n",
       " 'jinhone',\n",
       " 'jinka',\n",
       " 'jinke',\n",
       " 'jinki',\n",
       " 'jinn',\n",
       " 'jis',\n",
       " 'jise',\n",
       " 'jiska',\n",
       " 'jiske',\n",
       " 'jiski',\n",
       " 'jisme',\n",
       " 'jiss',\n",
       " 'jisse',\n",
       " 'jitna',\n",
       " 'jitne',\n",
       " 'jitni',\n",
       " 'jo',\n",
       " 'just',\n",
       " 'jyaada',\n",
       " 'jyada',\n",
       " 'k',\n",
       " 'ka',\n",
       " 'kaafi',\n",
       " 'kab',\n",
       " 'kabhi',\n",
       " 'kafi',\n",
       " 'kaha',\n",
       " 'kahaa',\n",
       " 'kahaan',\n",
       " 'kahan',\n",
       " 'kahi',\n",
       " 'kahin',\n",
       " 'kahte',\n",
       " 'kaisa',\n",
       " 'kaise',\n",
       " 'kaisi',\n",
       " 'kal',\n",
       " 'kam',\n",
       " 'kar',\n",
       " 'kara',\n",
       " 'kare',\n",
       " 'karega',\n",
       " 'karegi',\n",
       " 'karen',\n",
       " 'karenge',\n",
       " 'kari',\n",
       " 'karke',\n",
       " 'karna',\n",
       " 'karne',\n",
       " 'karni',\n",
       " 'karo',\n",
       " 'karta',\n",
       " 'karte',\n",
       " 'karti',\n",
       " 'karu',\n",
       " 'karun',\n",
       " 'karunga',\n",
       " 'karungi',\n",
       " 'kaun',\n",
       " 'kaunsa',\n",
       " 'kayi',\n",
       " 'kch',\n",
       " 'ke',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'keh',\n",
       " 'kehte',\n",
       " 'kept',\n",
       " 'khud',\n",
       " 'ki',\n",
       " 'kin',\n",
       " 'kine',\n",
       " 'kinhe',\n",
       " 'kinho',\n",
       " 'kinka',\n",
       " 'kinke',\n",
       " 'kinki',\n",
       " 'kinko',\n",
       " 'kinn',\n",
       " 'kino',\n",
       " 'kis',\n",
       " 'kise',\n",
       " 'kisi',\n",
       " 'kiska',\n",
       " 'kiske',\n",
       " 'kiski',\n",
       " 'kisko',\n",
       " 'kisliye',\n",
       " 'kisne',\n",
       " 'kitna',\n",
       " 'kitne',\n",
       " 'kitni',\n",
       " 'kitno',\n",
       " 'kiya',\n",
       " 'kiye',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'ko',\n",
       " 'koi',\n",
       " 'kon',\n",
       " 'konsa',\n",
       " 'koyi',\n",
       " 'krna',\n",
       " 'krne',\n",
       " 'kuch',\n",
       " 'kuchch',\n",
       " 'kuchh',\n",
       " 'kul',\n",
       " 'kull',\n",
       " 'kya',\n",
       " 'kyaa',\n",
       " 'kyu',\n",
       " 'kyuki',\n",
       " 'kyun',\n",
       " 'kyunki',\n",
       " 'lagta',\n",
       " 'lagte',\n",
       " 'lagti',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'le',\n",
       " 'least',\n",
       " 'lekar',\n",
       " 'lekin',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'li',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'liya',\n",
       " 'liye',\n",
       " 'll',\n",
       " 'lo',\n",
       " 'log',\n",
       " 'logon',\n",
       " 'lol',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'lunga',\n",
       " 'm',\n",
       " 'maan',\n",
       " 'maana',\n",
       " 'maane',\n",
       " 'maani',\n",
       " 'maano',\n",
       " 'magar',\n",
       " 'mai',\n",
       " 'main',\n",
       " 'maine',\n",
       " 'mainly',\n",
       " 'mana',\n",
       " 'mane',\n",
       " 'mani',\n",
       " 'mano',\n",
       " 'many',\n",
       " 'mat',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'mein',\n",
       " 'mera',\n",
       " 'mere',\n",
       " 'merely',\n",
       " 'meri',\n",
       " 'might',\n",
       " 'mightn',\n",
       " 'mightnt',\n",
       " \"mightn't\",\n",
       " 'mil',\n",
       " 'mjhe',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'mujhe',\n",
       " 'must',\n",
       " 'mustn',\n",
       " 'mustnt',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'na',\n",
       " 'naa',\n",
       " 'naah',\n",
       " 'nahi',\n",
       " 'nahin',\n",
       " 'nai',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'neeche',\n",
       " 'need',\n",
       " 'needn',\n",
       " 'neednt',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nhi',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nope',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'par',\n",
       " 'pata',\n",
       " 'pe',\n",
       " 'pehla',\n",
       " 'pehle',\n",
       " 'pehli',\n",
       " 'people',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'phla',\n",
       " 'phle',\n",
       " 'phli',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'poora',\n",
       " 'poori',\n",
       " 'provides',\n",
       " 'pura',\n",
       " 'puri',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'raha',\n",
       " 'rahaa',\n",
       " 'rahe',\n",
       " 'rahi',\n",
       " 'rakh',\n",
       " 'rakha',\n",
       " 'rakhe',\n",
       " 'rakhen',\n",
       " 'rakhi',\n",
       " 'rakho',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'rehte',\n",
       " 'rha',\n",
       " 'rhaa',\n",
       " 'rhe',\n",
       " 'rhi',\n",
       " 'ri',\n",
       " 'right',\n",
       " 's',\n",
       " 'sa',\n",
       " 'saara',\n",
       " 'saare',\n",
       " 'saath',\n",
       " 'sab',\n",
       " 'sabhi',\n",
       " 'sabse',\n",
       " 'sahi',\n",
       " 'said',\n",
       " 'sakta',\n",
       " 'saktaa',\n",
       " 'sakte',\n",
       " 'sakti',\n",
       " 'same',\n",
       " 'sang',\n",
       " 'sara',\n",
       " 'sath',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'se',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'shan',\n",
       " 'shant',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'shouldnt',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'si',\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'soch',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " 'tab',\n",
       " 'tabh',\n",
       " 'tak',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tarah',\n",
       " 'teen',\n",
       " 'teeno',\n",
       " 'teesra',\n",
       " 'teesre',\n",
       " 'teesri',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'tera',\n",
       " 'tere',\n",
       " 'teri',\n",
       " 'th',\n",
       " 'tha',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'theek',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thi',\n",
       " 'thik',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'third',\n",
       " 'this',\n",
       " 'tho',\n",
       " 'thoda',\n",
       " 'thodi',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'tjhe',\n",
       " 'to',\n",
       " 'together',\n",
       " 'toh',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tu',\n",
       " 'tujhe',\n",
       " 'tum',\n",
       " 'tumhara',\n",
       " 'tumhare',\n",
       " 'tumhari',\n",
       " 'tune',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'um',\n",
       " 'umm',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unhe',\n",
       " 'unhi',\n",
       " 'unho',\n",
       " 'unhone',\n",
       " 'unka',\n",
       " 'unkaa',\n",
       " 'unke',\n",
       " 'unki',\n",
       " 'unko',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'unn',\n",
       " 'unse',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upar',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'usi',\n",
       " 'using',\n",
       " 'uska',\n",
       " 'uske',\n",
       " 'usne',\n",
       " 'uss',\n",
       " 'usse',\n",
       " 'ussi',\n",
       " 'usually',\n",
       " 'vaala',\n",
       " 'vaale',\n",
       " 'vaali',\n",
       " 'vahaan',\n",
       " 'vahan',\n",
       " 'vahi',\n",
       " 'vahin',\n",
       " 'vaisa',\n",
       " 'vaise',\n",
       " 'vaisi',\n",
       " 'vala',\n",
       " 'vale',\n",
       " 'vali',\n",
       " 'various',\n",
       " 've',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vo',\n",
       " 'waala',\n",
       " 'waale',\n",
       " 'waali',\n",
       " 'wagaira',\n",
       " 'wagairah',\n",
       " 'wagerah',\n",
       " 'waha',\n",
       " 'wahaan',\n",
       " 'wahan',\n",
       " 'wahi',\n",
       " 'wahin',\n",
       " 'waisa',\n",
       " 'waise',\n",
       " 'waisi',\n",
       " 'wala',\n",
       " 'wale',\n",
       " 'wali',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'wasnt',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " 'weren',\n",
       " 'werent',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what's\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'with',\n",
       " 'within',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"hinglish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"cookbook\")[0]\n",
    "syn.name()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets(\"cookbook\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a book of recipes and cooking directions'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('contraction.n.01'),\n",
       " Synset('compression.n.02'),\n",
       " Synset('contraction.n.03'),\n",
       " Synset('contraction.n.04')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the devising of plans',\n",
       " 'the fashioning of pots and pans',\n",
       " 'the making of measurements',\n",
       " 'it was already in the making']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"making\")[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets(\"making\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('cookbook.n.01')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synset(\"cookbook.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My husband doesn't cook\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"cooking\")[1].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('reference_book.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('book.n.01')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('creation.n.02'),\n",
       "  Synset('product.n.02'),\n",
       "  Synset('work.n.02'),\n",
       "  Synset('publication.n.01'),\n",
       "  Synset('book.n.01'),\n",
       "  Synset('reference_book.n.01'),\n",
       "  Synset('cookbook.n.01')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernym_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets(\"great\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets(\"great\", pos=\"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordnet.synsets(\"great\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"cooking\")[0]\n",
    "lemmas = syn.lemmas()\n",
    "len(lemmas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooking'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].name() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookery'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[1].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preparation'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[2].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total synonyms of word Book In wordnet  =  38\n",
      "Total synonyms of word Book In wordnet  =  25\n",
      "Print All synonyms of Book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'book',\n",
       " 'volume',\n",
       " 'record',\n",
       " 'record_book',\n",
       " 'book',\n",
       " 'script',\n",
       " 'book',\n",
       " 'playscript',\n",
       " 'ledger',\n",
       " 'leger',\n",
       " 'account_book',\n",
       " 'book_of_account',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'rule_book',\n",
       " 'Koran',\n",
       " 'Quran',\n",
       " \"al-Qur'an\",\n",
       " 'Book',\n",
       " 'Bible',\n",
       " 'Christian_Bible',\n",
       " 'Book',\n",
       " 'Good_Book',\n",
       " 'Holy_Scripture',\n",
       " 'Holy_Writ',\n",
       " 'Scripture',\n",
       " 'Word_of_God',\n",
       " 'Word',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book',\n",
       " 'reserve',\n",
       " 'hold',\n",
       " 'book',\n",
       " 'book',\n",
       " 'book']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"book\"):\n",
    "  for lemma in syn.lemmas():\n",
    "    synonyms.append(lemma.name())\n",
    "print(\"Total synonyms of word Book In wordnet\",\" = \",len(synonyms))\n",
    "print(\"Total synonyms of word Book In wordnet\",\" = \",len(set(synonyms)))\n",
    "\n",
    "print(\"Print All synonyms of Book\")\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: Synset('good.n.01')\n",
      "Definition: benefit\n",
      "Lemmas:\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.n.02')\n",
      "Definition: moral excellence or admirableness\n",
      "Lemmas:\n",
      "- good\n",
      "  Antonyms: evil\n",
      "- goodness\n",
      "  Antonyms: evilness\n",
      "\n",
      "Synset: Synset('good.n.03')\n",
      "Definition: that which is pleasing or valuable or useful\n",
      "Lemmas:\n",
      "- good\n",
      "  Antonyms: bad\n",
      "- goodness\n",
      "  Antonyms: badness\n",
      "\n",
      "Synset: Synset('commodity.n.01')\n",
      "Definition: articles of commerce\n",
      "Lemmas:\n",
      "- commodity\n",
      "- trade_good\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.a.01')\n",
      "Definition: having desirable or positive qualities especially those suitable for a thing specified\n",
      "Lemmas:\n",
      "- good\n",
      "  Antonyms: bad\n",
      "\n",
      "Synset: Synset('full.s.06')\n",
      "Definition: having the normally expected amount\n",
      "Lemmas:\n",
      "- full\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.a.03')\n",
      "Definition: morally admirable\n",
      "Lemmas:\n",
      "- good\n",
      "  Antonyms: evil\n",
      "\n",
      "Synset: Synset('estimable.s.02')\n",
      "Definition: deserving of esteem and respect\n",
      "Lemmas:\n",
      "- estimable\n",
      "- good\n",
      "- honorable\n",
      "- respectable\n",
      "\n",
      "Synset: Synset('beneficial.s.01')\n",
      "Definition: promoting or enhancing well-being\n",
      "Lemmas:\n",
      "- beneficial\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.s.06')\n",
      "Definition: agreeable or pleasing\n",
      "Lemmas:\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.s.07')\n",
      "Definition: of moral excellence\n",
      "Lemmas:\n",
      "- good\n",
      "- just\n",
      "- upright\n",
      "\n",
      "Synset: Synset('adept.s.01')\n",
      "Definition: having or showing knowledge and skill and aptitude\n",
      "Lemmas:\n",
      "- adept\n",
      "- expert\n",
      "- good\n",
      "- practiced\n",
      "- proficient\n",
      "- skillful\n",
      "- skilful\n",
      "\n",
      "Synset: Synset('good.s.09')\n",
      "Definition: thorough\n",
      "Lemmas:\n",
      "- good\n",
      "\n",
      "Synset: Synset('dear.s.02')\n",
      "Definition: with or in a close or intimate relationship\n",
      "Lemmas:\n",
      "- dear\n",
      "- good\n",
      "- near\n",
      "\n",
      "Synset: Synset('dependable.s.04')\n",
      "Definition: financially sound\n",
      "Lemmas:\n",
      "- dependable\n",
      "- good\n",
      "- safe\n",
      "- secure\n",
      "\n",
      "Synset: Synset('good.s.12')\n",
      "Definition: most suitable or right for a particular purpose\n",
      "Lemmas:\n",
      "- good\n",
      "- right\n",
      "- ripe\n",
      "\n",
      "Synset: Synset('good.s.13')\n",
      "Definition: resulting favorably\n",
      "Lemmas:\n",
      "- good\n",
      "- well\n",
      "\n",
      "Synset: Synset('effective.s.04')\n",
      "Definition: exerting force or influence\n",
      "Lemmas:\n",
      "- effective\n",
      "- good\n",
      "- in_effect\n",
      "- in_force\n",
      "\n",
      "Synset: Synset('good.s.15')\n",
      "Definition: capable of pleasing\n",
      "Lemmas:\n",
      "- good\n",
      "\n",
      "Synset: Synset('good.s.16')\n",
      "Definition: appealing to the mind\n",
      "Lemmas:\n",
      "- good\n",
      "- serious\n",
      "\n",
      "Synset: Synset('good.s.17')\n",
      "Definition: in excellent physical condition\n",
      "Lemmas:\n",
      "- good\n",
      "- sound\n",
      "\n",
      "Synset: Synset('good.s.18')\n",
      "Definition: tending to promote physical well-being; beneficial to health\n",
      "Lemmas:\n",
      "- good\n",
      "- salutary\n",
      "\n",
      "Synset: Synset('good.s.19')\n",
      "Definition: not forged\n",
      "Lemmas:\n",
      "- good\n",
      "- honest\n",
      "\n",
      "Synset: Synset('good.s.20')\n",
      "Definition: not left to spoil\n",
      "Lemmas:\n",
      "- good\n",
      "- undecomposed\n",
      "- unspoiled\n",
      "- unspoilt\n",
      "\n",
      "Synset: Synset('good.s.21')\n",
      "Definition: generally admired\n",
      "Lemmas:\n",
      "- good\n",
      "\n",
      "Synset: Synset('well.r.01')\n",
      "Definition: (often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\n",
      "Lemmas:\n",
      "- well\n",
      "  Antonyms: ill\n",
      "- good\n",
      "\n",
      "Synset: Synset('thoroughly.r.02')\n",
      "Definition: completely and absolutely (`good' is sometimes used informally for `thoroughly')\n",
      "Lemmas:\n",
      "- thoroughly\n",
      "- soundly\n",
      "- good\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get synsets for the word \"good\"\n",
    "synsets = wordnet.synsets(\"good\")\n",
    "\n",
    "# Iterate over synsets and print antonyms for each lemma\n",
    "for synset in synsets:\n",
    "    print(\"Synset:\", synset)\n",
    "    print(\"Definition:\", synset.definition())\n",
    "    print(\"Lemmas:\")\n",
    "    for lemma in synset.lemmas():\n",
    "        print(\"-\", lemma.name())\n",
    "        # Print antonyms for each lemma\n",
    "        antonyms = lemma.antonyms()\n",
    "        if antonyms:\n",
    "            print(\"  Antonyms:\", \", \".join(ant.name() for ant in antonyms))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating WordNet Synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "cb = wordnet.synset(\"cookbook.n.01\")\n",
    "ib = wordnet.synset(\"instruction_book.n.01\")\n",
    "\n",
    "cb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = cb.hypernyms()[0]\n",
    "cb.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib.shortest_path_distance(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.shortest_path_distance(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38095238095238093"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wordnet.synsets(\"dog\")[0]\n",
    "dog.wup_similarity(cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('whole.n.02')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dog.common_hypernyms(cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Synset('dog.n.01')\n",
      "- Synset('frump.n.01')\n",
      "- Synset('dog.n.03')\n",
      "- Synset('cad.n.01')\n",
      "- Synset('frank.n.02')\n",
      "- Synset('pawl.n.01')\n",
      "- Synset('andiron.n.01')\n",
      "- Synset('chase.v.01')\n"
     ]
    }
   ],
   "source": [
    "dog = wordnet.synsets(\"dog\")\n",
    "\n",
    "for synset in dog:\n",
    "  print(\"-\",synset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  check how many sense numbers (senses) are available for a word using NLTK's WordNet interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses for 'cook': 5\n",
      "Number of senses for 'bake': 4\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Get synsets for the word \"cook\"\n",
    "synsets_cook = wordnet.synsets(\"cook\", pos='v')\n",
    "\n",
    "# Get synsets for the word \"bake\"\n",
    "synsets_bake = wordnet.synsets(\"bake\", pos='v')\n",
    "\n",
    "# Print the number of senses for each word\n",
    "print(\"Number of senses for 'cook':\", len(synsets_cook))\n",
    "print(\"Number of senses for 'bake':\", len(synsets_bake))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Synset('cook.n.01')\n",
      "- Synset('cook.n.02')\n",
      "- Synset('cook.v.01')\n",
      "- Synset('cook.v.02')\n",
      "- Synset('cook.v.03')\n",
      "- Synset('fudge.v.01')\n",
      "- Synset('cook.v.05')\n"
     ]
    }
   ],
   "source": [
    "cook = wordnet.synsets(\"cook\")\n",
    "\n",
    "for synset in cook:\n",
    "  print(\"-\",synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Synset('bake.v.01')\n",
      "- Synset('bake.v.02')\n",
      "- Synset('broil.v.02')\n",
      "- Synset('bake.v.04')\n"
     ]
    }
   ],
   "source": [
    "bake = wordnet.synsets(\"bake\")\n",
    "\n",
    "for synset in bake:\n",
    "  print(\"-\",synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "cook = wordnet.synset(\"cook.v.01\")\n",
    "bake = wordnet.synset(\"bake.v.02\")\n",
    "\n",
    "\n",
    "cook.wup_similarity(bake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.path_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_needs_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdog\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mistr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:871\u001b[0m, in \u001b[0;36mSynset.path_similarity\u001b[1;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpath_similarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, simulate_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    843\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;124;03m    Path Distance Similarity:\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    Return a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m        itself.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortest_path_distance(\n\u001b[0;32m    870\u001b[0m         other,\n\u001b[1;32m--> 871\u001b[0m         simulate_root\u001b[38;5;241m=\u001b[39msimulate_root \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_needs_root\u001b[49m()),\n\u001b[0;32m    872\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '_needs_root'"
     ]
    }
   ],
   "source": [
    "cb.path_similarity(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0281482472922856"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.lch_similarity(ib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Similarity: 0.2\n",
      "Leacock Chodorow Similarity: 2.0281482472922856\n",
      "Wu-Palmer Similarity: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Example synsets\n",
    "cb = wordnet.synset(\"cat.n.01\")\n",
    "ib = wordnet.synset(\"dog.n.01\")\n",
    "\n",
    "# Compute similarity using path_similarity\n",
    "path_sim = cb.path_similarity(ib)\n",
    "print(\"Path Similarity:\", path_sim)\n",
    "\n",
    "# Compute similarity using lch_similarity\n",
    "lch_sim = cb.lch_similarity(ib)\n",
    "print(\"Leacock Chodorow Similarity:\", lch_sim)\n",
    "\n",
    "# Compute similarity using wup_similarity\n",
    "wup_sim = cb.wup_similarity(ib)\n",
    "print(\"Wu-Palmer Similarity:\", wup_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stammer = PorterStemmer()\n",
    "stammer.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookeri'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"cookery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stammer = LancasterStemmer()\n",
    "stammer.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookery'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"cookery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "stammer = RegexpStemmer(\"ing\")\n",
    "\n",
    "stammer.stem(\"cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookery'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"cookery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leside'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"ingleside\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"cooking\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"cookbook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here's an example that illustrates one of the major differences between stemming and lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stammer = PorterStemmer()\n",
    "stammer.stem(\"believes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"believes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buse'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"buses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bus'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"buses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bu'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stammer.stem(\"bus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cannot is constration'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\\\g<1> would')\n",
    "]\n",
    "\n",
    "\n",
    "class RegexpReplacer:\n",
    "\n",
    "  def __init__(self, patterns=replacement_patterns):\n",
    "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "\n",
    "  def replace(self,text):\n",
    "    s = text\n",
    "    for (pattern,repl) in self.patterns:\n",
    "      s = re.sub(pattern,repl,s)\n",
    "    return s\n",
    "\n",
    "replacer = RegexpReplacer()\n",
    "\n",
    "replacer.replace(\"can't is constration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I should have done that thing I did not do'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"I should've done that thing I didn't do\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacement before tokenization\n",
    "Let's try using the RegexpReplacer class as a preliminary step before tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca', \"n't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "replacement_patterns = [\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', '\\\\g<1> would')\n",
    "]\n",
    "\n",
    "\n",
    "class RegexpReplacer:\n",
    "\n",
    "  def __init__(self, patterns=replacement_patterns):\n",
    "    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "\n",
    "  def replace(self,text):\n",
    "    s = text\n",
    "    for (pattern,repl) in self.patterns:\n",
    "      s = re.sub(pattern,repl,s)\n",
    "    return s\n",
    "  \n",
    "replacer = RegexpReplacer()\n",
    "\n",
    "word_tokenize(\"can't is a contraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'not', 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(replacer.replace(\"can't is a contraction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing repeating characters\n",
    "\n",
    "In everyday language, people are often not strictly grammatical. They will write things such as\n",
    "I looooooove it in order to emphasize the word love. However, computers don't know\n",
    "that \"looooooove\" is a variation of \"love\" unless they are told. This recipe presents a method\n",
    "to remove these annoying repeating characters in order to end up with a proper English word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "class RepeatReplacer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    self.repl = r'\\1\\2\\3'\n",
    "\n",
    "  def replace(self, word):\n",
    "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "    if repl_word != word:\n",
    "      return self.replace(repl_word)\n",
    "    else:\n",
    "      return repl_word\n",
    "    \n",
    "replacer = RepeatReplacer()\n",
    "replacer.replace(\"looooooove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"byyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"wwwoooooowwwwww\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gose'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"goooooooose\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding examples, you can see that the RepeatReplacer class is a bit too\n",
    "greedy and ends up changing goose into gose. To correct this issue, we can augment\n",
    "the replace() function with a WordNet lookup. If WordNet recognizes the word, then\n",
    "we can stop replacing characters. Here is the WordNet-augmented version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goose'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "class RepeatReplacer:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    self.repl = r'\\1\\2\\3'\n",
    "\n",
    "  def replace(self,word):\n",
    "    if wordnet.synsets(word):\n",
    "      return word\n",
    "    repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "    if repl_word != word:\n",
    "      return self.replace(repl_word)\n",
    "    else:\n",
    "      return repl_word    \n",
    "\n",
    "replacer = RepeatReplacer()\n",
    "\n",
    "replacer.replace(\"ooooohhhhh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goose'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"goooose\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"woooowww\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling correction with Enchant\n",
    "\n",
    "Replacing repeating characters is actually an extreme form of spelling correction. In this\n",
    "recipe, we will take on the less extreme case of correcting minor spelling issues using\n",
    "Enchanta spelling correction API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook\n",
      "believe\n",
      "\n",
      "The edit distance is the number of character changes\n",
      "necessary to transform the given word into the suggested word.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer:\n",
    "\n",
    "    def __init__(self, dict_name=\"en_US\", max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_distance = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_distance:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "\n",
    "# Example usage:\n",
    "replacer = SpellingReplacer()\n",
    "replaced_word = replacer.replace(\"cookbok\")\n",
    "print(replaced_word)\n",
    "\n",
    "replaced_word = replacer.replace(\"belyeve\")\n",
    "print(replaced_word)\n",
    "\n",
    "print(\"\"\"\\nThe edit distance is the number of character changes\n",
    "necessary to transform the given word into the suggested word.\"\"\")\n",
    "edit_distance(\"language\",\"languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'languages', 'langrage']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enchant\n",
    "\n",
    "d = enchant.Dict(\"en\")\n",
    "d.suggest('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en_BW',\n",
       " 'en_AU',\n",
       " 'en_BZ',\n",
       " 'en_GB',\n",
       " 'en_JM',\n",
       " 'en_DK',\n",
       " 'en_HK',\n",
       " 'en_GH',\n",
       " 'en_US',\n",
       " 'en_ZA',\n",
       " 'en_ZW',\n",
       " 'en_SG',\n",
       " 'en_NZ',\n",
       " 'en_BS',\n",
       " 'en_AG',\n",
       " 'en_PH',\n",
       " 'en_IE',\n",
       " 'en_NA',\n",
       " 'en_TT',\n",
       " 'en_IN',\n",
       " 'en_NG',\n",
       " 'en_CA']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enchant.list_languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing synonyms\n",
    "\n",
    "We'll first create a WordReplacer class in replacers.py that takes a word replacement mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birthday\n",
      "Machine Learning\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "class SpellingReplacer:\n",
    "\n",
    "    def __init__(self, dict_name=\"en_US\", max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_distance = max_dist\n",
    "\n",
    "    def replace(self, word):\n",
    "        if self.spell_dict.check(word):\n",
    "            return word\n",
    "        suggestions = self.spell_dict.suggest(word)\n",
    "\n",
    "        if suggestions and edit_distance(word, suggestions[0]) <= self.max_distance:\n",
    "            return suggestions[0]\n",
    "        else:\n",
    "            return word\n",
    "        \n",
    "class customSpellingReplacer(SpellingReplacer):\n",
    "    \n",
    "    def __init__(self,spell_dict,max_dict=2):\n",
    "        self.spell_dict = spell_dict\n",
    "        self.max_dict = max_dict\n",
    "\n",
    "class WordReplacer(object):\n",
    "  def __init__(self,word_map):\n",
    "    self.word_map = word_map\n",
    "\n",
    "  def replace(self,word):\n",
    "    return self.word_map.get(word,word)\n",
    "\n",
    "word_map = WordReplacer({\"bday\":\"birthday\"})\n",
    "replacer1 = WordReplacer({\"ML\":\"Machine Learning\"})\n",
    "replacer2 = WordReplacer({\"happy\": \"happy\"})\n",
    "\n",
    "print(word_map.replace(\"bday\"))\n",
    "\n",
    "print(replacer1.replace(\"ML\"))\n",
    "\n",
    "print(replacer2.replace(\"happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in word_map: 3\n"
     ]
    }
   ],
   "source": [
    "class WordReplacer(object):\n",
    "    def __init__(self, word_map):\n",
    "        self.word_map = word_map\n",
    "\n",
    "    def replace(self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "\n",
    "    def word_count(self):\n",
    "        return len(self.word_map)\n",
    "\n",
    "# Example usage:\n",
    "replacer = WordReplacer({\"bday\": \"birthday\", \"hello\": \"hi\", \"goodbye\": \"bye\"})\n",
    "print(\"Number of words in word_map:\", replacer.word_count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautify'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "\n",
    "  def replace(self,word,pos=None):\n",
    "    antonyms = set()\n",
    "\n",
    "    for syn in wordnet.synsets(word,pos=pos):\n",
    "      for lemma in syn.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "          antonyms.add(antonym.name())\n",
    "    \n",
    "    if len(antonyms) == 1:\n",
    "      return antonyms.pop()\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "\n",
    "  def replace_Negations(self, sent):\n",
    "    i, l = 0, len(sent)\n",
    "    words = []\n",
    "\n",
    "    while i < l:\n",
    "        word = sent[i]\n",
    "        if word == \"not\":\n",
    "            if i + 1 < l:\n",
    "                ant = self.replace(sent[i + 1]) \n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "            # If \"not\" is not followed by any more words, append it as it is\n",
    "            words.append(word)\n",
    "        else:\n",
    "            words.append(word)\n",
    "        i += 1\n",
    "    return words\n",
    "\n",
    "    \n",
    "replacer = AntonymReplacer()\n",
    "\n",
    "replacer.replace(\"uglify\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"let's\", 'beautify', 'our', 'code']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = [\"let's\", 'not', 'uglify', 'our', 'code'] \n",
    "\n",
    "replacer.replace_Negations(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'is', 'good']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AntonymWordReplacer(WordReplacer, AntonymReplacer):\n",
    "  pass\n",
    "\n",
    "replacer = AntonymWordReplacer({\"evil\":\"good\"})\n",
    "\n",
    "replacer.replace_Negations(['good', 'is', 'not', 'evil'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter 3 Creating Custom Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,os.path\n",
    "\n",
    "path = os.path.expanduser(\"~/nltk_data\")\n",
    "\n",
    "if not os.path.exists(path):\n",
    "  os.mkdir(path)\n",
    "\n",
    "os.path.exists(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "path in nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() got an unexpected keyword argument 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/mistr/nltk_data/mywords.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'path'"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.data.load(path=\"C:/Users/mistr/nltk_data/mywords.txt\", format=\"raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk', 'corpus', 'corpora', 'wordnet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "reader = WordListCorpusReader(\".\",[\"wordlist.txt\"])\n",
    "reader.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nltk\\r\\ncorpus\\r\\ncorpora\\r\\nwordnet'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk', 'corpus', 'corpora', 'wordnet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import line_tokenize\n",
    "line_tokenize(reader.raw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  English words corpus\n",
    "\n",
    "NLTK also comes with a large list of English words. There's one file with 850 basic words,\n",
    "and another list with over 200,000 known English words, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en', 'en-basic']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# English words corpus\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "words.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.words(\"en-basic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235886"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words.words(\"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is not working\n",
    "from nltk.corpus.reader import TaggedCorpusReader\n",
    "\n",
    "reader = TaggedCorpusReader('.', r'.*\\.pos')\n",
    "fileids = reader.fileids()  # Get all fileids\n",
    "words = reader.words(fileids=fileids)  # Get words from all files\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code is not working it give lits of errors\n",
    "\n",
    "# from nltk.corpus.reader import ChunkedCorpusReader\n",
    "\n",
    "# reader = ChunkedCorpusReader('.', r'.*\\.chunk')\n",
    "# fileids = reader.fileids()  # Get all fileids\n",
    "# chunked_words = reader.chunked_words(fileids=fileids)  # Get chunked words from all files\n",
    "# print(chunked_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have skipped chapter 3 Creating Custom Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tagging is the process of converting a sentence, in the form of a list of words,\n",
    "into a list of tuples, where each tuple is of the form (word, tag). The tag is a part-of-speech\n",
    "tag, and signifies whether the word is a noun, adjective, verb, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "pos_tag = nltk.pos_tag(words)\n",
    "\n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'), ('world', 'NN'), ('tejash', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "tagger = DefaultTagger(\"NN\")\n",
    "\n",
    "tagger.tag([\"hello\",\"world\",\"tejash\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_12284\\3330351125.py:4: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14331966328512843"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('hello', 'NN'), ('world', 'NN'), ('.', 'NN')],\n",
       " [('how', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag_sents([[\"hello\",\"world\",\".\"],[\"how\",\"are\",\"you\",\"?\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untagging a tagged sentence\n",
    "Tagged sentences can be untagged using nltk.tag.untag(). Calling this function with\n",
    "a tagged sentence will return a list of words without the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'World']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import untag\n",
    "\n",
    "untag([('Hello', 'NN'), ('World', 'NN')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a unigram part-of-speech tagger\n",
    "\n",
    "A unigram generally refers to a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "treebank.sents()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_12284\\2463806165.py:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', None),\n",
       " ('Vinken', None),\n",
       " (',', None),\n",
       " ('61', None),\n",
       " ('years', None),\n",
       " ('old', None),\n",
       " (',', None),\n",
       " ('will', None),\n",
       " ('join', None),\n",
       " ('the', None),\n",
       " ('board', None),\n",
       " ('as', None),\n",
       " ('a', None),\n",
       " ('nonexecutive', None),\n",
       " ('director', None),\n",
       " ('Nov.', None),\n",
       " ('29', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overriding the context model \n",
    "\n",
    "tagger = UnigramTagger(model={\"pierre\":\"NN\"})\n",
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum frequency cutoff\n",
    "The ContextTagger class uses frequency of occurrence to decide which tag is most likely\n",
    "for a given context. By default, it will do this even if the context word and tag occurs only once.\n",
    "If you'd like to set a minimum frequency threshold, then you can pass a cutoff value to the\n",
    "UnigramTagger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_12284\\2304075332.py:2: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.775350744657889"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(train_sents,cutoff=3)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mistr\\AppData\\Local\\Temp\\ipykernel_12284\\639207555.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  tagger2.evaluate(test_sents)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8741204403194475"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger1 = DefaultTagger(\"NN\")\n",
    "tagger2 = UnigramTagger(train_sents,backoff=tagger1)\n",
    "tagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger1._taggers == [tagger1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger2._taggers == [tagger2,tagger1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page No 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
